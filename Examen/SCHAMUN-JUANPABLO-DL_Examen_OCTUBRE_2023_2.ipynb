{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXnmgdtOuZs"
      },
      "source": [
        "# Universidad de Buenos Aires\n",
        "# Deep Learning - Examen\n",
        "# Octubre 2023\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkTaSvR85KJ6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEx1gM2sG4OP"
      },
      "source": [
        "El examen comienza al momento de recibir este correo y la ventana de entrega estará abierta hasta el Lunes 23 de Octubre a las 20:00hs. Toda comunicación con otros alumnos respecto del examen y la resolución de los ejercicios, queda estrictamente prohibida. Los exámenes serán comparados desde el punto de vista de la redacción, de los resultados y del código para determinar que el trabajo fue 100% individual y único. El examen es a libro abierto, pudiendo utilizar los contenidos vistos en clase y otra bibliografía. Todas las soluciones deben ser originales y si se toman ideas de fuentes externas deben ser correctamente citas incluyendo el correspondiente link o página de libro.\n",
        "\n",
        "El formato de entrega debe ser un “link a un colab” (compartir a las siguientes direcciones: maxit1992@gmail.com y lelectronfou@gmail.com ) o un “link a un notebook en un github público”.\n",
        "\n",
        "**Consideraciones a tener en cuenta:**\n",
        "- Se entregará 1 solo colab para la totalidad del examen.\n",
        "- Renombrar el archivo de la siguiente manera: **APELLIDO-NOMBRE-DL-Examen OCTUBRE 2023.ipynb**\n",
        "- Los códigos deben poder ejecutarse.\n",
        "- Los resultados, cómo el código y las explicaciones deben quedar guardados y visualizables en el correspondiente link.\n",
        "- Prestar atención a las consignas, responder las preguntas cuando corresponda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlfSxrehmuYW"
      },
      "source": [
        "## Ejercicio 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K24O_YBWUIP"
      },
      "source": [
        "Dado el grafo de cómputo de la siguiente imagen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-uXfqO9WWx2"
      },
      "source": [
        "![Graph.jpg](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwCiDRncfsaI"
      },
      "source": [
        "Donde a=1, b=2 y c=3\n",
        "\n",
        "1. Realizar el paso forward y calcular Y.\n",
        "\n",
        "2. Realizar el paso backward y calcular el backpropagation para a,b y c:\n",
        "  \n",
        "    a. En forma analítica (dy/dx = 0)\n",
        "  \n",
        "    b. Aplicando regla de la cadena\n",
        "\n",
        "    c. Utilizando pytorch\n",
        "\n",
        "3. Comparar los resultados del punto 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# inicio de inputs\n",
        "a = np.random.random()\n",
        "b = np.random.random()\n",
        "c = np.random.random()\n",
        "\n",
        "print(f'a: {a}')\n",
        "print(f'b: {b}')\n",
        "print(f'c: {c}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inciso 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1 Forward\n",
        "d = a + b\n",
        "e = b - c\n",
        "Y = d*e\n",
        "\n",
        "print (f'Y = {Y}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inciso 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2_a\n",
        "# Derivadas Parciales Analitica\n",
        "\n",
        "\n",
        "dY_da = (b - c)\n",
        "dY_db = a + 2*b -c\n",
        "dY_dc = -(a + b)\n",
        "\n",
        "# Igualanado a 0 las derivadas parciales no se se obtienen\n",
        "# valores únicos. Queda indeterminado\n",
        "# b = c = -a\n",
        "\n",
        "print(f'dY/da = {dY_da}')\n",
        "print(f'dY/db = {dY_db}')\n",
        "print(f'dY/dc = {dY_dc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2_b Derivadas Parciales Regla de la cadena\n",
        "\n",
        "dY_dd = e # dY/dd\n",
        "dY_de = d # dY/de\n",
        "\n",
        "dd_da =  1 # dd/da\n",
        "dd_db =  1 # dd/db\n",
        "de_db =  1 # de/db\n",
        "de_dc = -1 # de/dc\n",
        "\n",
        "dY_da = dY_dd*dd_da                # dY/da\n",
        "dY_db = dY_dd*dd_db + dY_de*de_db  # dY/db\n",
        "dY_dc = dY_de*de_dc                # dY/dc  \n",
        "\n",
        "print(f'dY/da = {dY_da}')\n",
        "print(f'dY/db = {dY_db}')\n",
        "print(f'dY/dc = {dY_dc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2_c Derivadas Parciales utilizando Pytorch\n",
        "\n",
        "a = torch.tensor([a], requires_grad=True)\n",
        "b = torch.tensor([b], requires_grad=True)\n",
        "c = torch.tensor([c], requires_grad=True)\n",
        "\n",
        "print(f'a: {a}')\n",
        "print(f'b: {b}')\n",
        "print(f'c: {c}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward\n",
        "d = a + b\n",
        "e = b - c\n",
        "Y = d*e\n",
        "\n",
        "print (f'Y = {Y}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1.])\n",
        "Y.backward(external_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dY_da = a.grad # dY/da\n",
        "dY_db = b.grad # dY/db\n",
        "dY_dc = c.grad # dY/dc \n",
        "\n",
        "print(f'dY/da = {dY_da}')\n",
        "print(f'dY/db = {dY_db}')\n",
        "print(f'dY/dc = {dY_dc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inciso 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En todos los casos las derivadas dan lo mismo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvsqmJwx_Rb8"
      },
      "source": [
        "## Ejercicio 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABeIRvAkKLy7"
      },
      "source": [
        "a) Implemente un modelo de red recurrente con la arquitectura presentada en la imagen. La misma debe tener:\n",
        "\n",
        "\n",
        "* Como input una señal de 4 canales (4 features)\n",
        "* 3 capas ocultas con 20 neuronas.\n",
        "* Tenga como salida una señal de 2 canales (2 features) de igual longitud que la señal de entrada. (ayuda! emplee 2 capas recurrentes independientes)\n",
        "\n",
        "\n",
        "b) Testeear el modelo con una señal random de longitud 20.\n",
        "\n",
        "c) Indique el total de parámetros entrenables.\n",
        "\n",
        "\n",
        "\n",
        "![c](https://drive.google.com/uc?export=view&id=1jDMr1hOiHBgkDZepHfOaxWQcaNNntB7W)\n",
        "\n",
        "\n",
        "\n",
        "Imagen 3 link (https://drive.google.com/file/d/1jDMr1hOiHBgkDZepHfOaxWQcaNNntB7W/view?usp=drive_link)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNN_clas(torch.nn.Module):\n",
        "  def __init__(self, input_size=2, hidden_size=40, num_layers=2, out_fc=5):\n",
        "    super().__init__()\n",
        "    self.rnn1 = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_size, out_fc )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, h = self.rnn1(x)\n",
        "    o_fc = self.fc(x[:,-1]) # le paso el hidden state final de la rrn1\n",
        "                      # esto será de largo igual al numero de hidden\n",
        "                      # tiene sentido pasar el hidden final por que tendrá\n",
        "                      # la información de la secuencia en entrada ya \"almacenada\"\n",
        "    # o_fc_prob = torch.softmax(o_fc, 1)\n",
        "    return o_fc, o_fc # acá repito la salida ya que mi funcion \"teoria\"\n",
        "                                # espera que el modelo arroje 2 salidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def imp_param(model):\n",
        "  print('-'*84)\n",
        "  print('PARAMETROS DEL MODELO')\n",
        "  print('-'*84)\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print('Nombre del parámetro: ')\n",
        "      print(name)\n",
        "      print('Tamaño del parámetro: ')\n",
        "      print(param.data.shape)\n",
        "      print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def teoria(model, largo_entrada = 3, batch_size=1):\n",
        "  print('-'*84)\n",
        "  print('MODELO')\n",
        "  print('-'*84)\n",
        "  print(model)\n",
        "  imp_param(model)\n",
        "\n",
        "  # Generamos una entrada aleatoria para ver como responde la red\n",
        "  # el tamaño de la entrada esa acorde a los tamaños que cargamos antes\n",
        "  entrada = torch.rand(batch_size, largo_entrada, input_size)\n",
        "  print('-'*84)\n",
        "  print('ENTRADA')\n",
        "  print('-'*84)\n",
        "  print('entrada shape: ', entrada.shape)\n",
        "  print(entrada)\n",
        "\n",
        "  # le agrego la dimension del batch:\n",
        "  #entrada = entrada[None, :]\n",
        "  print()\n",
        "  print('entrada con nuevas dimensiones [batch, Length, nr_features] ')\n",
        "  print(entrada.shape)\n",
        "\n",
        "  # Pasamos la entrada a la red\n",
        "  o, h = model(entrada)\n",
        "  print('-'*84)\n",
        "  print('SALIDA')\n",
        "  print('-'*84)\n",
        "  print('salida de la red (output) (largo igual al input): ', o.shape)\n",
        "  print(o)\n",
        "  print()\n",
        "  print('hidden red (solo ultimo hidden): ', h.shape)\n",
        "  print(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size= 4\n",
        "batch_size=1\n",
        "hidden_size=20\n",
        "num_layers=3\n",
        "\n",
        "largo_entrada = 20\n",
        "\n",
        "model = RNN_clas(input_size, hidden_size, num_layers)\n",
        "teoria(model, largo_entrada, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL2PjUnT_Uvk"
      },
      "source": [
        "## Ejercicio 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73yqE_Sslwif"
      },
      "source": [
        "Se desea construir un sistema de recomendación de películas. Para esto se cuenta con un dataset de las puntuaciones que los usuarios han asignado a las peliculas disponibles.\n",
        "\n",
        "Link dataset: https://drive.google.com/file/d/1Og9H-8oqb3_Wo_WOakeAuRR_mwr922Ar/view?usp=sharing\n",
        "\n",
        "Para verificar la factibilidad del proyecto con datos válidos, se decide utilizar solamente las 200 películas con más votos y los usuarios que han puntuado al menos 100 películas.\n",
        "\n",
        "1- Analizar el dataset para utilizar solamente las 200 películas con mayor cantidad de votos y los usuarios que hayan votado al menos 100 películas.\n",
        "\n",
        "2- A partir del dataset del punto 1, construir una única red neuronal que utilice una capa de embeddings para el id de usuario, una capa de embeddings para el id de película y al menos dos capas lineales que sea capaz de predecir el puntaje que cada usuario colocó a cada pelicula.\n",
        "\n",
        "3- Elegir un usuario al azar, una película que dicho usuario haya puntuado y verificar la predicción del modelo. Comparar con el puntaje real que el usuario asignó a dicha película.\n",
        "\n",
        "4- Realizar una recomendación de película para el usuario del punto 3 utilizando los embeddings de usuario o los embeddings de películas. Comprobar si la recomendación es correcta haciendo una predicción del puntuaje con la red neuronal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eLQzdNeRjzym"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.-  Armado del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(610,)\n",
            "(9724,)\n",
            "(100836, 4)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('ratings.csv')\n",
        "print(df['userId'].unique().shape)\n",
        "print(df['movieId'].unique().shape)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(248,)\n",
            "(200,)\n",
            "(18588, 4)\n"
          ]
        }
      ],
      "source": [
        "# Filtrado\n",
        "pelis = df.groupby(['movieId'])['userId'].count().sort_values(ascending=False)[:200].index\n",
        "df1 = df.groupby(['userId'])['movieId'].count()\n",
        "usuarios = df1[df1 >= 100].index\n",
        "dfFiltrado = df[(df['userId'].isin(usuarios)) & (df['movieId'].isin(pelis))]\n",
        "\n",
        "print(dfFiltrado['userId'].unique().shape)\n",
        "print(dfFiltrado['movieId'].unique().shape)\n",
        "print(dfFiltrado.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.- Red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divido el dataset\n",
        "idx = np.random.permutation(dfFiltrado.shape[0])\n",
        "train_idx = idx[0:int(0.85*len(idx))]\n",
        "valid_idx = idx[int(0.85*len(idx)):]\n",
        "\n",
        "# armamos una array con los datos de entrenamiento y validacion\n",
        "\n",
        "pelis    = np.array(dfFiltrado.iloc[idx]['movieId'])\n",
        "usuarios = np.array(dfFiltrado.iloc[idx]['userId'])\n",
        "rating   = np.array(dfFiltrado.iloc[idx]['rating'])\n",
        "\n",
        "pelis_train = pelis[train_idx] \n",
        "users_train = usuarios[train_idx]\n",
        "rating_train = rating[train_idx]\n",
        "                      \n",
        "pelis_valid = pelis[valid_idx] \n",
        "users_valid = usuarios[valid_idx]\n",
        "rating_valid = rating[valid_idx]\n",
        "\n",
        "\n",
        "n_train = rating.shape[0]\n",
        "n_valid = rating.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rating.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diccionario de vocabulario Pelis\n",
            "{1: 0, 33794: 1, 2: 2, 6: 3, 10: 4, 2571: 5, 1036: 6, 527: 7, 16: 8, 19: 9, 21: 10, 539: 11, 541: 12, 32: 13, 34: 14, 551: 15, 39: 16, 3114: 17, 1580: 18, 47: 19, 1584: 20, 1073: 21, 50: 22, 1080: 23, 2617: 24, 68157: 25, 1089: 26, 2115: 27, 2628: 28, 1097: 29, 586: 30, 587: 31, 588: 32, 589: 33, 3147: 34, 590: 35, 592: 36, 593: 37, 1610: 38, 1617: 39, 1101: 40, 597: 41, 595: 42, 95: 43, 608: 44, 5218: 45, 104: 46, 110: 47, 111: 48, 1136: 49, 1653: 50, 2683: 51, 2174: 52, 4226: 53, 648: 54, 48780: 55, 141: 56, 2706: 57, 1682: 58, 150: 59, 153: 60, 2716: 61, 44191: 62, 161: 63, 165: 64, 60069: 65, 8360: 66, 1193: 67, 1704: 68, 1196: 69, 1197: 70, 1198: 71, 1200: 72, 8368: 73, 1206: 74, 5816: 75, 1721: 76, 1208: 77, 185: 78, 1210: 79, 1213: 80, 1214: 81, 58559: 82, 7361: 83, 1219: 84, 1732: 85, 1221: 86, 1222: 87, 1220: 88, 2762: 89, 208: 90, 3793: 91, 4306: 92, 1240: 93, 6874: 94, 6365: 95, 1246: 96, 223: 97, 736: 98, 733: 99, 5349: 100, 231: 101, 2791: 102, 6377: 103, 1258: 104, 1259: 105, 2797: 106, 750: 107, 1265: 108, 1270: 109, 1784: 110, 253: 111, 8961: 112, 5378: 113, 260: 114, 778: 115, 1291: 116, 780: 117, 4878: 118, 7438: 119, 2324: 120, 788: 121, 4886: 122, 2329: 123, 1307: 124, 79132: 125, 4896: 126, 288: 127, 292: 128, 293: 129, 72998: 130, 296: 131, 2858: 132, 5418: 133, 2355: 134, 3897: 135, 316: 136, 318: 137, 5952: 138, 5445: 139, 329: 140, 32587: 141, 1356: 142, 339: 143, 344: 144, 858: 145, 68954: 146, 2396: 147, 349: 148, 4963: 149, 356: 150, 5989: 151, 2916: 152, 2918: 153, 357: 154, 1387: 155, 364: 156, 3948: 157, 3949: 158, 367: 159, 1391: 160, 1393: 161, 4973: 162, 377: 163, 380: 164, 1917: 165, 4993: 166, 4995: 167, 48516: 168, 1923: 169, 904: 170, 6539: 171, 2959: 172, 912: 173, 919: 174, 410: 175, 3996: 176, 924: 177, 1961: 178, 4011: 179, 2987: 180, 1968: 181, 434: 182, 59315: 183, 2997: 184, 4022: 185, 4027: 186, 2502: 187, 454: 188, 457: 189, 2011: 190, 2012: 191, 480: 192, 2028: 193, 1517: 194, 7153: 195, 5618: 196, 500: 197, 1527: 198, 3578: 199}\n",
            "diccionario de vocabulario Usuarios\n",
            "{1: 0, 514: 1, 4: 2, 517: 3, 6: 4, 7: 5, 520: 6, 10: 7, 522: 8, 524: 9, 525: 10, 15: 11, 527: 12, 17: 13, 18: 14, 19: 15, 20: 16, 21: 17, 534: 18, 23: 19, 22: 20, 24: 21, 27: 22, 28: 23, 542: 24, 32: 25, 33: 26, 39: 27, 40: 28, 551: 29, 42: 30, 555: 31, 41: 32, 45: 33, 43: 34, 559: 35, 47: 36, 561: 37, 560: 38, 562: 39, 564: 40, 52: 41, 51: 42, 567: 43, 50: 44, 57: 45, 58: 46, 59: 47, 570: 48, 573: 49, 62: 50, 63: 51, 64: 52, 577: 53, 66: 54, 572: 55, 580: 56, 68: 57, 73: 58, 586: 59, 587: 60, 74: 61, 76: 62, 590: 63, 80: 64, 593: 65, 82: 66, 83: 67, 84: 68, 597: 69, 596: 70, 599: 71, 600: 72, 601: 73, 602: 74, 91: 75, 604: 76, 605: 77, 606: 78, 95: 79, 608: 80, 607: 81, 610: 82, 603: 83, 100: 84, 103: 85, 104: 86, 105: 87, 109: 88, 111: 89, 113: 90, 115: 91, 117: 92, 119: 93, 122: 94, 125: 95, 129: 96, 132: 97, 135: 98, 136: 99, 137: 100, 139: 101, 140: 102, 141: 103, 144: 104, 153: 105, 156: 106, 160: 107, 166: 108, 167: 109, 169: 110, 177: 111, 181: 112, 182: 113, 184: 114, 186: 115, 187: 116, 195: 117, 198: 118, 199: 119, 200: 120, 201: 121, 202: 122, 210: 123, 212: 124, 216: 125, 217: 126, 552: 127, 219: 128, 220: 129, 221: 130, 222: 131, 226: 132, 230: 133, 232: 134, 233: 135, 234: 136, 239: 137, 240: 138, 246: 139, 247: 140, 249: 141, 254: 142, 256: 143, 260: 144, 263: 145, 265: 146, 266: 147, 268: 148, 563: 149, 274: 150, 275: 151, 279: 152, 280: 153, 282: 154, 286: 155, 287: 156, 288: 157, 290: 158, 292: 159, 294: 160, 298: 161, 301: 162, 304: 163, 305: 164, 306: 165, 307: 166, 308: 167, 309: 168, 312: 169, 313: 170, 314: 171, 571: 172, 317: 173, 318: 174, 322: 175, 325: 176, 326: 177, 328: 178, 330: 179, 331: 180, 332: 181, 334: 182, 339: 183, 346: 184, 351: 185, 352: 186, 354: 187, 356: 188, 357: 189, 362: 190, 365: 191, 367: 192, 368: 193, 369: 194, 372: 195, 373: 196, 376: 197, 377: 198, 380: 199, 381: 200, 382: 201, 385: 202, 387: 203, 391: 204, 393: 205, 405: 206, 408: 207, 409: 208, 410: 209, 411: 210, 412: 211, 414: 212, 419: 213, 420: 214, 424: 215, 425: 216, 428: 217, 594: 218, 432: 219, 434: 220, 436: 221, 437: 222, 438: 223, 448: 224, 89: 225, 452: 226, 453: 227, 462: 228, 464: 229, 465: 230, 466: 231, 469: 232, 474: 233, 475: 234, 477: 235, 479: 236, 480: 237, 482: 238, 483: 239, 484: 240, 488: 241, 489: 242, 490: 243, 492: 244, 495: 245, 509: 246, 510: 247}\n"
          ]
        }
      ],
      "source": [
        "# armamos un diccionario que me mapee el un índex\n",
        "# para pelis\n",
        "peli_to_idx = {peliId: i for i, peliId in enumerate(set(pelis))}\n",
        "# Vector de pelis_idx en el dataset\n",
        "pelis_idx = np.array([peli_to_idx[value] for value in pelis])\n",
        "# Divido el vector pelis_idx en entrenamiento y validación\n",
        "pelis_idx_train = pelis_idx[train_idx]\n",
        "pelis_idx_valid = pelis_idx[valid_idx]\n",
        "\n",
        "# para usuarios\n",
        "user_to_idx = {userId: i for i, userId in enumerate(set(usuarios))}\n",
        "# Vector de users_idx en el dataset\n",
        "users_idx = np.array([user_to_idx[value] for value in usuarios])\n",
        "# Divido el vector users_idx en entrenamiento y validación\n",
        "users_idx_train = users_idx[train_idx]\n",
        "users_idx_valid = users_idx[valid_idx]\n",
        "\n",
        "print('diccionario de vocabulario Pelis')\n",
        "print(peli_to_idx)\n",
        "print('diccionario de vocabulario Usuarios')\n",
        "print(user_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clase Dataset de Pytorch con embeddings\n",
        "class MyDatasetWithEmbddings(Dataset):\n",
        "\n",
        "  def __init__(self, movieId, userId, y):\n",
        "    \n",
        "    self.movieId = movieId\n",
        "    self.userId = userId\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.movieId[idx], self.userId[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds_embeddings = MyDatasetWithEmbddings(pelis_idx_train, users_idx_train, rating_train)\n",
        "valid_ds_embeddings = MyDatasetWithEmbddings(pelis_idx_valid, users_idx_valid, rating_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader_emb = DataLoader(train_ds_embeddings, batch_size =64, shuffle= True)\n",
        "valid_dataloader_emb = DataLoader(valid_ds_embeddings, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class red(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size1=50, embedding_dim1=2, vocab_size2=50, embedding_dim2=2):\n",
        "        super(red, self).__init__()\n",
        "\n",
        "        self.embeddings1 = torch.nn.Embedding(vocab_size1, embedding_dim1)\n",
        "        self.embeddings2 = torch.nn.Embedding(vocab_size2, embedding_dim2)        \n",
        "        self.linear1 = torch.nn.Linear(embedding_dim1 + embedding_dim2, 200, bias=True)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200, 100)\n",
        "        self.act2 = torch.nn.ReLU()\n",
        "        self.output = torch.nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, peli, usuario):\n",
        "        embeds1 = self.embeddings1(peli)\n",
        "        embeds2 = self.embeddings2(usuario)\n",
        "        final_input = torch.cat([embeds1, embeds2], dim=1)\n",
        "        out = self.linear1(final_input)\n",
        "        out = self.act1(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.act2(out)\n",
        "        out = self.output(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.0809404   1.2456456   0.4662779  ... -1.0735015   0.8025843\n",
            "   0.38976142]\n",
            " [ 0.9336026   0.42097637 -0.04546918 ... -0.01018359  0.31203952\n",
            "   2.0437315 ]\n",
            " [-0.2105526  -0.33396316 -0.7586162  ... -0.03909083 -0.48202476\n",
            "  -0.2371599 ]\n",
            " ...\n",
            " [ 1.9005368   1.4509728   1.0389491  ... -2.0624588   0.2242704\n",
            "  -0.1709513 ]\n",
            " [ 1.0949656   0.63935775 -0.98553485 ...  1.0447768   1.7740213\n",
            "  -0.29552552]\n",
            " [-0.8421991   0.6551594   0.09325999 ... -0.6782989  -0.36446285\n",
            "   0.28512797]]\n",
            "[[ 1.5186557  -0.1922825  -0.43463814 ...  1.0101777   1.9459563\n",
            "  -0.9151496 ]\n",
            " [-0.22019194  0.40357772  0.5299693  ... -0.9967523   0.52801573\n",
            "  -0.6033157 ]\n",
            " [ 0.87008584 -0.99031365 -0.18185756 ... -1.1318277   0.97022754\n",
            "   0.6471074 ]\n",
            " ...\n",
            " [-4.0296116   1.1117419   0.15249422 ...  2.0063312  -0.9300063\n",
            "   0.8721974 ]\n",
            " [ 0.0951079  -0.9582844   1.9441952  ... -0.32107013  1.5711945\n",
            "  -0.47995   ]\n",
            " [-1.2064666  -0.9942754  -1.0735556  ... -0.9740713  -1.4328907\n",
            "  -1.1121833 ]]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "model = red(vocab_size1=len(set(pelis)), embedding_dim1=8, vocab_size2=len(set(usuarios)), embedding_dim2=8)\n",
        "\n",
        "# si quiero cargar un modelo ya entrenado:\n",
        "# model.load_state_dict(torch.load(os.path.join('./', \"embeddings_pelisYusersEmbSize8.pt\"), map_location=device))\n",
        "\n",
        "loss_function = torch.nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=0.01) #, momentum=0.1)\n",
        "\n",
        "emb_partida1 = np.copy(model.embeddings1.weight.detach().numpy())\n",
        "emb_partida2 = np.copy(model.embeddings2.weight.detach().numpy())\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(emb_partida1)\n",
        "print(emb_partida2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32md:\\Users\\juanp_schamun\\Documents\\GitRepositories\\CEIA\\aprendizaje_profundo\\Examen\\SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb Cell 44\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m train_loss_by_epoch\u001b[39m.\u001b[39mappend(epoch_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Cálculo la métrica de la epoch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m accuracy \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49maccuracy_score(epoch_y, [j\u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m \u001b[39mfor\u001b[39;49;00m j \u001b[39min\u001b[39;49;00m epoch_y_hat])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m############################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m## Validación\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m############################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Desactivo el cálculo de gradiente para validación\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y142sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Users\\juanp_schamun\\Anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
            "File \u001b[1;32md:\\Users\\juanp_schamun\\Anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32md:\\Users\\juanp_schamun\\Anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
            "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
          ]
        }
      ],
      "source": [
        "# Mini-Batch\n",
        "\n",
        "# cantidad de epochs\n",
        "epochs = 100\n",
        "\n",
        "train_loss_by_epoch=[]\n",
        "valid_loss_by_epoch=[]\n",
        "\n",
        "# Doble loop algoritmo Mini-Batch\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  ############################################\n",
        "  ## Entrenamiento\n",
        "  ############################################\n",
        "  model.train(True)\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_y_hat = []\n",
        "  epoch_y = []\n",
        "  \n",
        "  for i,data in enumerate(train_dataloader_emb):\n",
        "    # Obtengo los datos del batch de entrenamiento\n",
        "    embed_batch1, embed_batch2, y_batch = data\n",
        "    # Copio el batch al dispositivo donde entreno la red neuronal\n",
        "    embed_batch1 = embed_batch1.to(device).int()\n",
        "    embed_batch2 = embed_batch2.to(device).int()\n",
        "    y_batch = y_batch.to(device).float().reshape(-1, 1)\n",
        "\n",
        "    # Paso forward\n",
        "    # Limpio optimizer para empezar un nuevo cálculo de gradiente\n",
        "    optimizer.zero_grad()\n",
        "    nnet_output = model(embed_batch1, embed_batch2)\n",
        "    y_batch_hat = torch.sigmoid(nnet_output)\n",
        "    \n",
        "    # Calculo el loss\n",
        "    loss = loss_function(nnet_output, y_batch)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Actualizar los parámetros\n",
        "    optimizer.step()\n",
        "\n",
        "    # Almaceno los valores reales y mis predicciones para cálcular las métricas\n",
        "    epoch_y += list(y_batch.detach().cpu().numpy())\n",
        "    epoch_y_hat += list(y_batch_hat.detach().cpu().numpy())\n",
        "    # Acumulo la loss del batch\n",
        "    epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "  # Calculo la media de la loss\n",
        "  epoch_loss = epoch_loss / n_train\n",
        "  # Almaceno la loss de la epoch para graficar\n",
        "  train_loss_by_epoch.append(epoch_loss)\n",
        "  # Cálculo la métrica de la epoch\n",
        "  accuracy = metrics.accuracy_score(epoch_y, [j>=0.5 for j in epoch_y_hat])\n",
        "\n",
        "  ############################################\n",
        "  ## Validación\n",
        "  ############################################\n",
        "  # Desactivo el cálculo de gradiente para validación\n",
        "  model.train(False)\n",
        "\n",
        "  valid_epoch_loss = 0\n",
        "  valid_epoch_y_hat = []\n",
        "  valid_epoch_y = []\n",
        "\n",
        "  for i,data in enumerate(valid_dataloader_emb):\n",
        "    # Obtengo los datos del batch de validación\n",
        "    embed_batch1, embed_batch2, y_batch = data\n",
        "    # Copio el batch al dispositivo donde entreno la red neuronal\n",
        "    embed_batch1 = embed_batch1.to(device).int()\n",
        "    embed_batch2 = embed_batch2.to(device).int()\n",
        "    y_batch = y_batch.to(device).float().reshape(-1, 1)\n",
        "\n",
        "    # Paso forward\n",
        "    nnet_output = model(embed_batch1, embed_batch2)\n",
        "    y_batch_hat = torch.sigmoid(nnet_output)\n",
        "    \n",
        "    # Calculo el loss\n",
        "    loss = loss_function(nnet_output, y_batch)\n",
        "\n",
        "    # En validación no hago backpropagation!!\n",
        "\n",
        "    # Almaceno los valores reales y mis predicciones para cálcular las métricas\n",
        "    valid_epoch_y += list(y_batch.detach().cpu().numpy())\n",
        "    valid_epoch_y_hat += list(y_batch_hat.detach().cpu().numpy())\n",
        "    # Acumulo la loss del batch\n",
        "    valid_epoch_loss = valid_epoch_loss + loss.item()\n",
        "\n",
        "  # Calculo la media de la loss\n",
        "  valid_epoch_loss = valid_epoch_loss / n_valid\n",
        "  # Almaceno la loss de la epoch para graficar\n",
        "  valid_loss_by_epoch.append(valid_epoch_loss)\n",
        "  # Cálculo la métrica de la epoch\n",
        "  valid_accuracy = metrics.accuracy_score(valid_epoch_y, [j>=0.5 for j in valid_epoch_y_hat])\n",
        "\n",
        "  ############################################\n",
        "  ## Impresión de resultados por epoch\n",
        "  ############################################\n",
        "  print(f\" Epoch {epoch} | \" \\\n",
        "        f\"Train/Valid loss: {epoch_loss:.3f} / {valid_epoch_loss:.3f} | \" \\\n",
        "        f\"Train/Valid accuracy: {accuracy:.3f} / {valid_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32md:\\Users\\juanp_schamun\\Documents\\GitRepositories\\CEIA\\aprendizaje_profundo\\Examen\\SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb Cell 46\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# le paso al modelo la input (pelis y usuarios para embeddings)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m pred \u001b[39m=\u001b[39m model(pelis_idxs, user_idxs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# calculo la loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(pred\u001b[39m.\u001b[39mfloat(), target\u001b[39m.\u001b[39mfloat())\n",
            "File \u001b[1;32md:\\Users\\juanp_schamun\\Anaconda3\\envs\\pytorchgpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32md:\\Users\\juanp_schamun\\Documents\\GitRepositories\\CEIA\\aprendizaje_profundo\\Examen\\SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m embeds1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings1(peli)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m embeds2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings2(usuario)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m final_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([embeds1, embeds2], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1(final_input)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/juanp_schamun/Documents/GitRepositories/CEIA/aprendizaje_profundo/Examen/SCHAMUN-JUANPABLO-DL_Examen_OCTUBRE_2023_2.ipynb#Y143sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact1(out)\n",
            "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "global BEST_VAL\n",
        "BEST_VAL = 99999\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    largo = len(x_y)\n",
        "    # armo un vector de indices random para leer\n",
        "    # mi matriz de datos de forma aleatoria\n",
        "    index_rand = np.random.permutation(largo)\n",
        "    for i in range(largo): # barro todo de set de entrenamiento\n",
        "      # saco el índice de ese vector de indices random\n",
        "      indice = index_rand[i]\n",
        "      # saco pelis y users y le paso a idx      \n",
        "      peli_i = x_y[indice,0]\n",
        "      user_i = x_y[indice,1]\n",
        "      pelis_idxs = torch.tensor(peli_to_idx[peli_i], dtype=torch.long).to(device)\n",
        "      user_idxs = torch.tensor(user_to_idx[user_i], dtype=torch.long).to(device)\n",
        "\n",
        "      # saco las otras variables\n",
        "      # x =  torch.tensor([[x_y[indice,1]]], dtype=torch.long)\n",
        "\n",
        "      # saco el valor deseado\n",
        "      target = torch.tensor([[x_y[indice,2]]]).to(device)\n",
        "\n",
        "      # preparo el  modelo (limpio el gradiente)\n",
        "      model.zero_grad()\n",
        "\n",
        "      # le paso al modelo la input (pelis y usuarios para embeddings)\n",
        "      pred = model(pelis_idxs, user_idxs)\n",
        "\n",
        "      # calculo la loss\n",
        "      loss = loss_function(pred.float(), target.float())\n",
        "\n",
        "      # Step 5. Do the backward pass and update the gradient\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    losses.append(total_loss) # para cada epoch, guardo la loss\n",
        "    \n",
        "    # Guardo el modelo\n",
        "    direccion = './'    \n",
        "    if total_loss < BEST_VAL:\n",
        "      BEST_VAL = total_loss\n",
        "      #salvado = os.path.join(path, \"User/Desktop\", \"file.txt\")\n",
        "      torch.save(model.state_dict(), os.path.join(direccion, \"embeddings_pelisYusersEmbSize8.pt\"))\n",
        "      print('Save Best Model in HISTORY\\n')\n",
        "      # imprimo resultados cada 1 epochs\n",
        "    if (int(epoch) % 1) == 0:\n",
        "        print(total_loss/largo*100)\n",
        "        # print(model.embeddings.weight[word_to_ix[1]])\n",
        "        # print(model.linear1.weight[0])\n",
        "#print(losses)  # The loss decreased every iteration over the training data!\n",
        "plt.plot(np.array(losses)/largo*100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "direccion = './'\n",
        "global BEST_VAL\n",
        "#if TRAIN_SCRATCH and avg_loss < BEST_VAL:\n",
        "#BEST_VAL = avg_loss\n",
        "#salvado = os.path.join(path, \"User/Desktop\", \"file.txt\")\n",
        "torch.save(model.state_dict(), os.path.join(direccion, \"embeddings_pelisYusers.pt\"))\n",
        "print('Save Best Model in HISTORY\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(os.path.join(direccion, \"embeddings_pelisYusers.pt\"), map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  1,   4,   6,   7,  10,  15,  17,  18,  19,  20,  21,  22,  23,\n",
              "        24,  27,  28,  32,  33,  39,  40,  41,  42,  43,  45,  47,  50,\n",
              "        51,  52,  57,  58,  59,  62,  63,  64,  66,  68,  73,  74,  76,\n",
              "        80,  82,  83,  84,  89,  91,  95, 100, 103, 104, 105, 109, 111,\n",
              "       113, 115, 117, 119, 122, 125, 129, 132, 135, 136, 137, 139, 140,\n",
              "       141, 144, 153, 156, 160, 166, 167, 169, 177, 181, 182, 184, 186,\n",
              "       187, 195, 198, 199, 200, 201, 202, 210, 212, 216, 217, 219, 220,\n",
              "       221, 222, 226, 230, 232, 233, 234, 239, 240, 246, 247, 249, 254,\n",
              "       256, 260, 263, 265, 266, 268, 274, 275, 279, 280, 282, 286, 287,\n",
              "       288, 290, 292, 294, 298, 301, 304, 305, 306, 307, 308, 309, 312,\n",
              "       313, 314, 317, 318, 322, 325, 326, 328, 330, 331, 332, 334, 339,\n",
              "       346, 351, 352, 354, 356, 357, 362, 365, 367, 368, 369, 372, 373,\n",
              "       376, 377, 380, 381, 382, 385, 387, 391, 393, 405, 408, 409, 410,\n",
              "       411, 412, 414, 419, 420, 424, 425, 428, 432, 434, 436, 437, 438,\n",
              "       448, 452, 453, 462, 464, 465, 466, 469, 474, 475, 477, 479, 480,\n",
              "       482, 483, 484, 488, 489, 490, 492, 495, 509, 510, 514, 517, 520,\n",
              "       522, 524, 525, 527, 534, 542, 551, 552, 555, 559, 560, 561, 562,\n",
              "       563, 564, 567, 570, 571, 572, 573, 577, 580, 586, 587, 590, 593,\n",
              "       594, 596, 597, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608,\n",
              "       610], dtype=int64)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['userId'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([    1,     6,    47,    50,   110,   223,   231,   260,   296,\n",
              "         316,   349,   356,   367,   457,   480,   500,   527,   590,\n",
              "         592,   593,   608,   648,   733,   736,   780,   919,  1073,\n",
              "        1080,  1089,  1097,  1136,  1196,  1197,  1198,  1206,  1208,\n",
              "        1210,  1213,  1214,  1219,  1220,  1222,  1240,  1258,  1265,\n",
              "        1270,  1291,  1517,  1580,  1617,  1732,  2012,  2028,  2115,\n",
              "        2174,  2329,  2502,  2571,  2617,  2628,  2716,  2797,  2858,\n",
              "        2916,  2959,  2987,  2997,  3147,  3578,  3793,    21,    32,\n",
              "         357,   539,   588,   595,   904,   912,  1259,  1391,  1704,\n",
              "        1923,  1968,  2324,  2683,  2762,  2791,  3897,  3996,  4027,\n",
              "        4226,  4896,     2,    10,    16,    19,    34,    95,   104,\n",
              "         141,   150,   153,   161,   165,   185,   208,   253,   288,\n",
              "         292,   293,   318,   329,   339,   344,   364,   377,   380,\n",
              "         410,   434,   454,   587,   589,   597,   750,   924,  1101,\n",
              "        1246,  1584,  1610,  1682,  1784,  1917,  3114,  4306,  4886,\n",
              "        4963,  4993,  4995,  5218,  5349,  5378,  5445,  5618,  5816,\n",
              "        5952,  5989,  6365,  6539,  7153,  8360,  8368,  8961, 32587,\n",
              "       33794, 48516,  1307,  6377, 58559, 68954, 72998,   858,  1200,\n",
              "        1527,  1653,  2011,  3949,  4022,  6874,  7438, 48780, 59315,\n",
              "       60069, 79132,   111,   541,  1036,  1193,  1221, 68157,   586,\n",
              "         778,  1356,  1721,  1961,  2706,  2918,  4011,  4973,  5418,\n",
              "        7361, 44191,   551,   788,  2355,  2396,  3948,  4878,  1387,\n",
              "          39,  1393], dtype=int64)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['movieId'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.1643]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)\n",
        "pelis_idxs = torch.tensor(peli_to_idx[47], dtype=torch.long).to(device)\n",
        "user_idxs = torch.tensor(user_to_idx[15], dtype=torch.long).to(device)\n",
        "model(pelis_idxs,user_idxs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
